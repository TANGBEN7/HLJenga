# 机器人学导论开发日志



[TOC]

## 环境配置

* SDK和库
  * SDK（比如realsense SDK）
    * 软件开发工具包，指辅助开发某一类软件的相关文档、范例和工具的集合
    * SDK提供的是业务应用领域的功能
    * SDK产品一般是企业针对其具体业务设计开发，是企业对外提供服务的手段之一
    * 完整的SDK产品，包括完善的文档说明、使用实例、测试用例等
    * 一般会像程序一样安装在`program files`
  * 库（比如opencv和eigen）
    * 用于开发软件的子程序集合
    * 库提供的代码是比较底层的功能
    * 库不是独立程序，他们是向其他程序提供服务的代码。库是封装好的代码，通过调用开放出来的API获取相应的功能
    * 想放哪里放哪里，可以放到vs工作空间或者工程文件夹中

* realsense sdk 2.0

  * 下载安装

    > [Intel RealSense SDK 2.0 – Intel RealSense Depth and Tracking cameras](https://www.intelrealsense.com/sdk-2/)

  * 配置系统环境变量为`sdkroot/bin/x64`

    * 注意win的UI下的路径是`\`，程序里可用`/`

* opencv 4.5.2

  * 想试一下新版本的特性所以选了4

  * 资源在这儿

    > [win10 中 OpenCV4.5.2 的安装与环境配置](https://blog.csdn.net/ZChen1996/article/details/115985925)

  * 配置系统环境变量为`opencv/build/x64/vc15/bin`以及`lib`文件夹

* vs工程配置不完全示例
  * vs配置为release x64模式
  * opencv头文件：右边栏工程名右键、属性、VC++包含目录`opencv/build/include`
  * realsense头文件：`realsenseRoot/include`和`samples`
  * eigen头文件：`eigenRoot`
  * opencv库文件：VC++库目录`opencv/build/x64/vc15/lib`
  * realsense库文件：`realsenseRoot/lib/x64`
  * 依赖项：链接器、输入、附加依赖项：`opencv_world452.lib`以及`realsense2.lib`

## 手眼标定

* 首先在工程里创建了`HECalib.hpp`和`HECalib.cpp`。其中`HECalib.cpp`是从资源包里面的`eyeinhand.cpp`拷贝过来的。然后创建了`HECalibData`的文件夹，存放标定需要的图片，里面的子目录对应的机器人序号

* `eyeinhand.cpp`是有问题的

  * 没有`#include <fstream>`
  * 用的`CV_RGB2GRAY`和`opencv4`的兼容性问题，可以`#include <opencv2/imgproc/types_c.h>`解决
  * 代码缩进一团糟
  * 没有写对应的头文件

* 将接口写入`HECalib.hpp`

  * `#pragma once`防止被多次编译

  * 不能在头文件中使用`using namespace cv;`

    > 什么原因导致的？
    > 最后经过查找资料发现，是因为我项目中的其中一个头文件（.h）中使用了using namspace cv空间命名，但是我在源文件中又引用了系统头文件#include<windows.h>，这样就会导致了冲突。原因是他们俩中都有ACCESS_MASK定义，所以就会导致该变量不明确，就会报错。
    >
    > 解决办法
    > 最方便的解决办法可能就是，将所有的头文件（.h）中的using namespace cv都去掉，然后将需要用到的地方用cv::代替。然后将using namespace cv放入到cpp文件中，在cpp文件中不会导致冲突。所以就只会在.h文件中麻烦一点。

  * 把`eye_in_hand()`改成了`eye_in_hand(cv::String DATAPATH)`。是的没错，`String`是`cv`里的，小写的`string`才是`std`里的

* 在主函数中试着调用`eye_in_hand()`

  * 有一次`DATAPATH`没给对，出现了错误，发现竟然没有在没找到文件的时候`return`于是给他加上了
  * 路径正确的话，用资源包给的图像数据，可以在控制台中看到输出，并且在`HECalibData`文件夹下看见`Result.txt`

* 标定数据的获取

  * 移动机械臂，相机位置应尽量覆盖整个以标定板为中心的半球面
  * 取大概15个点，用realsense viewer保存`1280x720`分辨率的图像，同时用上位机软件获取机械臂末端位姿。第一个点尽量保持正视
  * 图片依照例子放在`HECalibData`里，位姿在`rpy.txt`里
  * 调用函数，得到`Result.txt`
  
* 关于标定的一些名词的意义

  * 内参代表了物体由从像素坐标系转换为相机坐标系的参数
  * 外参代表了相机坐标系转换为世界坐标系的参数
  * 手眼标定即标定相机坐标系到机械臂工具坐标系的变换参数，`Result.txt`最后的结果矩阵的意义是即相机相对于机械臂末端的位姿

## 矩形识别

* 创建`RectDetect`的头文件和源文件，复制参考代码
  * opencv4没有`cv.h`已经融合进了`imgproc`，去掉即可
  * 把类的声明写进头文件，再把函数的申明写进去就好
  * 还有些宏定义也挪过去了
  * 修改了`ColorDect`参数，调用时可以指定图片路径了
  
* 重要的一些函数

  * `Get_RGB()`连接realsense相机保存一个文件到`RectDetectData/detect.jpg`
  * `ColorDect()`如果不指定路径的话就默认识别`RectDetectData/detect.jpg`，并把结果保存到同文件夹下的`result.jpg`。关注 `My_rec`类的对象的`w_vertex`，`w_center`，`w_theta`这些矩形的世界系下的坐标
    
    * `ColorDect()`得到像素坐标系下的顶点位置，调用`rec()`，`uv_to_xyz()`算得世界系下的坐标
    
    * 注意`w_center`的`z`坐标需要根据实际机器人调整比较方便
    
    * 识别出来的量可能要加偏置才能用，对于2号机器人`x+=7; y-=4; theta+=0;`且角度需要加一个限位`-51~129`
    
    * 方便起见，把文件输出到`detectResult.txt`格式如下
    
      ```
      3
      424.133,103.96,455,0,180,50.6446
      494.173,-97.5971,455,0,180,-48.5438
      582.699,-60.8385,455,0,180,71.5313
      ```
    
      > 第一行表示有几个积木
      >
      > 下面是积木的位置x,y,z,thetaz1,thetay,thetaz2

* 实际识别要配合标定使用

  * 得到的标定结果关注下面这几个数据
    * 第一张图的相机坐标系下的平移矩阵，即对应位姿情况下相机坐标系原点到标定板中心的距离。第三个元素即为相机坐标系原点到标定板平面的垂直高度`z`
    * 相机与末端之间的相对位姿矩阵`R`
    * 相机内参矩阵
    
  * 在 `pixel_to_camera`里令`z`为标定得到的那个数据减去积木高度和标定板高度的差（并且写入相机内参矩阵`K`？）
  
    * 单位是毫米（即标定结果要乘1000）
    * 积木的厚度是15，标定板的厚度是4
    * 如果最后初始位置要往上移动，则要加上对应的距离，本项目中使用200
  
  * 在`camera_to_world`里面写入`T`矩阵，为机械臂当前位姿状态下相机的外参，可通过机械臂的末端位姿`g`和手眼标定矩阵`R`计算得到`T = gR`
  
    > 内参代表了物体由从像素坐标系转换为相机坐标系的参数
    >
    > 外参代表了相机坐标系转换为世界坐标系的参数
    >
    > 手眼标定即标定相机坐标系到机械臂工具坐标系的变换参数，`Result.txt`最后的结果矩阵的意义是即相机相对于机械臂末端的位姿

## 宏指令

* 一堆移植工作，此处大部分略过

  * 其中`poco`库只找到64位debug版本的，骚操作一番
    * 把d后缀的`lib`文件给复制成非d的
    * 拷贝d后缀的`dll`到`slnRoot/x64`下

* 用自己的电脑和程序连接机器人

  * 在控制面板，网络和internet选项中对Ethernet选项右键，双击ipv4项目设置

    ```
    IP = 192.168.10.5
    Mask = 255.255.255.0
    ```

* 参考`《QKM 机器人指令手册》`


## 目标点世界笛卡尔坐标系位置（2号机）

* 特殊点`(X, Y, Z, z, y, z)`，单位是毫米和角度

  * 拍照初始点`(400, 0, 700, 0, 180, 39)`

* 后续放置点（积木的三维是`15x25x75`）

  * 基座1层

    ```
    (680, 0, 455, 0, 180, 39)
    (680, 50, 455, 0, 180, 39)
    ```

  * 基座2层

    ```
    (655, 25, 470, 0, 180, 129)
    (705, 25, 470, 0, 180, 129)
    ```

  * 基座3层

    ```
    (680, 0, 485, 0, 180, 39)
    (680, 50, 485, 0, 180, 39)
    ```
  
  * 接着往上，`z`坐标每次往上移动15就ok
  
    ```
    (680, 25, 500, 0, 180, 129)...
    ```
  
* 一组用于测试的积木位置

  ```
  (398.737, 0040.8392, 455, 0, 180, 122)
  (447.627, -22.096, 455, 0, 180, -15)
  (506.11, -101.484, 455, 0, 180, -45)
  ```


## 22点41分啥都没实现离结题不到48H

* 出现了`ftp`的问题，估计是`poco`库的环境问题，最后还有主函数没写完，已经实现带过点的LFPB规划，夜还很长，要做的事情

  * ~~增加最后一个维度的规划~~
  * ~~更改相对路径~~
  * ~~完成主函数~~
  * 更改为debug
    * 改为debug后确实可以运行，但是速度慢了很多
    * 试着自己编译poco，没见过这么麻烦的库，后面有时间得记一下
    * 可以在资源管理器中访问ftp地址，具体是`ftp://192.168.10.101/data`若看到上传的数据就可以认为成功


## 终于可以实机调试了

* ftp没有问题

* 规划部分的已知问题

  * start和end数值不能完全一样
  
  * xyz data last dim is in Joint space
  
  * 更改规划时间会报错
  
  * 点位个数有时会少
  
  * 几乎都是一个地方出错，这个参数39就很迷
  
    ```
    SetRobotEndPos(x_plan[i], y_plan[i], z_plan[i], 0, 180, 39, conf);
    ```
  
  * 规划成功了也必定出`overspeed`的问题

## 15点39分好起来了

* 到这里已经可以完成基础任务了
* 剩下的问题
  * 时间优化：修改参数和最后的放置位置，可以用较短的时间配置并且不会overspeed（done）
  * 最后会带起来：~~最后一个关节一开始和末尾不偏转~~，或者Z也可以加一个offset（done）
  * 代码规范问题：等最后的完整版再改一下
  * ~~不同机器的标定问题~~：找到了备用的吸盘（done）
  * 视觉识别的鲁棒性：~~轮廓排序~~、测试内参矩阵、把识别出来的offset给个define（done）

## 草。结题时间往后了，还能再加点功能

* 主函数和视觉部分
  * 断开相机？
  * 窗口大小（done）
  * 距离编号（done）
  * 测试内参矩阵（没用）
* 规划部分
  * 第零个轨迹
  * 各种时间优化
  * 最终根基位置

## June4 4:28PM算是完全整完了

* 之后再最后确认一下就好目前版本是`V4-RC0`
* 自己编译只需要vs打开文件夹，设置版本然后安装就行，如果提示出现没有vc什么版本，在vs installer里面安装对应的就好
* 可以问问助教怎么解决相机连接的问题

## June5 3:52PM HLJengaV4.0-Release 发布

* 效果不错，除了相机没有解决其他都解决了，顺便把规划的最后一块拼图给拼上了
* 稍微注意一下摆放的位置就行
* ~~算是把这机子玩懂了~~
